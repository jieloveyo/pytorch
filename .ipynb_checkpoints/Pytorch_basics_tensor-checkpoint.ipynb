{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 基础 : 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量(Tensor)\n",
    "张量的英文是Tensor，它是PyTorch里面基础的运算单位，与Numpy的ndarray相同都表示的是一个多维的矩阵。  \n",
    "与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。\n",
    "\n",
    "生成一个简单的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5821, 0.3839, 0.6767],\n",
       "        [0.3717, 0.2753, 0.4947]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3) #可以任意生成想要的size\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 可以使用与numpy相同的shape属性查看\n",
    "print(x.shape)\n",
    "# 也可以使用size()函数，返回的结果都是相同的\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量（Tensor）是一个定义在一些向量空间和一些对偶空间的笛卡儿积上的多重线性映射，**其坐标是|n|维空间内，有|n|个分量的一种量**， 其中每个分量都是坐标的函数， 而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。 (来自百度百科)\n",
    "\n",
    "下面生成一些多维的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5812, 0.9824, 0.3700, 0.8703, 0.3258],\n",
       "          [0.6868, 0.1223, 0.5715, 0.4778, 0.2254],\n",
       "          [0.1033, 0.9976, 0.2032, 0.4555, 0.4622],\n",
       "          [0.4424, 0.3466, 0.2975, 0.7486, 0.5034]],\n",
       "\n",
       "         [[0.2153, 0.8870, 0.9458, 0.7590, 0.9892],\n",
       "          [0.6646, 0.8095, 0.8229, 0.6213, 0.2789],\n",
       "          [0.4950, 0.6469, 0.9424, 0.9903, 0.4882],\n",
       "          [0.6367, 0.4505, 0.2655, 0.4938, 0.3228]],\n",
       "\n",
       "         [[0.8728, 0.1016, 0.8158, 0.6020, 0.2553],\n",
       "          [0.7104, 0.6706, 0.7550, 0.6073, 0.6986],\n",
       "          [0.4620, 0.2667, 0.3874, 0.9640, 0.1291],\n",
       "          [0.8478, 0.3837, 0.4778, 0.3089, 0.0982]]],\n",
       "\n",
       "\n",
       "        [[[0.0512, 0.1912, 0.6992, 0.8461, 0.0591],\n",
       "          [0.6329, 0.3485, 0.7167, 0.6537, 0.7594],\n",
       "          [0.0191, 0.7220, 0.9443, 0.0559, 0.6593],\n",
       "          [0.3591, 0.7648, 0.7301, 0.3838, 0.5566]],\n",
       "\n",
       "         [[0.1145, 0.0201, 0.7754, 0.4440, 0.5454],\n",
       "          [0.6924, 0.4307, 0.8845, 0.1813, 0.1253],\n",
       "          [0.8363, 0.0817, 0.2392, 0.7139, 0.7814],\n",
       "          [0.5602, 0.9659, 0.2389, 0.8544, 0.5597]],\n",
       "\n",
       "         [[0.2186, 0.0106, 0.4476, 0.3323, 0.9170],\n",
       "          [0.0834, 0.8944, 0.2445, 0.8280, 0.4373],\n",
       "          [0.7483, 0.9215, 0.4706, 0.1725, 0.9106],\n",
       "          [0.5248, 0.9701, 0.5208, 0.9734, 0.5320]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(2,3,4,5)\n",
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在同构的意义下:\n",
    "* 第零阶张量 （r = 0） 为标量 （Scalar）\n",
    "* 第一阶张量 （r = 1） 为向量 （Vector)\n",
    "* 第二阶张量 （r = 2） 则成为矩阵 （Matrix)\n",
    "* 第三阶以上的统称为多维张量。\n",
    "\n",
    "其中要特别注意的就是标量，我们先生成一个标量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3333)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = torch.tensor(2.333332)\n",
    "print(scalar)\n",
    "scalar.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于标量，我们可以直接使用 .item() 从中取出其对应的python对象的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.333332061767578"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别的：如果张量中只有一个元素的tensor也可以调用`tensor.item`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1433])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([3.1433223]) \n",
    "print(tensor)\n",
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.143322229385376"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本类型\n",
    "Tensor的基本数据类型有五种：\n",
    "- 32位浮点型：torch.FloatTensor。 (默认)\n",
    "- 64位整型：torch.LongTensor。\n",
    "- 32位整型：torch.IntTensor。\n",
    "- 16位整型：torch.ShortTensor。\n",
    "- 64位浮点型：torch.DoubleTensor。\n",
    "\n",
    "除以上数字类型外，还有\n",
    "byte和chart型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = tensor.long()\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1426], dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half = tensor.half()\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_t = tensor.int()\n",
    "int_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1433])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flo = tensor.float()\n",
    "flo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = tensor.short()\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = tensor.char()\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = tensor.byte()\n",
    "bt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy转换\n",
    "使用numpy方法将Tensor转为ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9762767   0.16459851]\n",
      " [-0.03374688  1.0314187 ]\n",
      " [ 0.5441837  -1.5202681 ]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 2))\n",
    "# tensor转化为numpy\n",
    "numpy_a = a.numpy()\n",
    "print(numpy_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy转化为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9763,  0.1646],\n",
       "        [-0.0337,  1.0314],\n",
       "        [ 0.5442, -1.5203]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a = torch.from_numpy(numpy_a)\n",
    "torch_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备间转换\n",
    "一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_a=torch.rand(4, 3)\n",
    "cpu_a.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_a=cpu_a.cuda()\n",
    "gpu_a.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.cpu方法将tensor移动到cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_b=gpu_a.cpu()\n",
    "cpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们有多GPU的情况，可以使用to方法来确定使用那个设备，这里只做个简单的实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用torch.cuda.is_available()来确定是否有cuda设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#将tensor传送到设备\n",
    "gpu_b=cpu_b.to(device)\n",
    "gpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "Pytorch中有许多默认的初始化方法可以使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3508, 0.2281, 0.7103],\n",
       "        [0.7015, 0.0825, 0.1847],\n",
       "        [0.7947, 0.9190, 0.3153],\n",
       "        [0.1924, 0.2055, 0.2896],\n",
       "        [0.0606, 0.4937, 0.9842]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用[0,1]均匀分布随机初始化二维数组\n",
    "rnd = torch.rand(5, 3)\n",
    "rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化，使用1填充\n",
    "one = torch.ones(2, 2)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化，使用0填充\n",
    "zero=torch.zeros(2,2)\n",
    "zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化一个单位矩阵，即对角线为1 其他为0\n",
    "eye=torch.eye(2,2)\n",
    "eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用方法\n",
    "PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么他们二者基本是一致的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6535,  0.4191,  0.2873],\n",
      "        [ 0.7946,  2.1109,  0.3434],\n",
      "        [-1.2967, -1.4868, -0.4521]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3) #不同的rand，取得是不同的，正态分布什么之类的\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.max()`函数\n",
    "* 简单来说是返回一个tensor中的最大值\n",
    "* torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\n",
    "\n",
    "### dim\n",
    "dim的用法不展开，可上网百度查，基本来说：dim=0 ==> 返回每一列的最大值；dim = 1 ==> 返回每一行的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6535,  2.1109, -0.4521]) tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# 沿着行取最大值\n",
    "max_value, max_idx = torch.max(x, dim=1)\n",
    "print(max_value, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3598,  3.2490, -3.2356])\n"
     ]
    }
   ],
   "source": [
    "# 每行 x 求和\n",
    "sum_x = torch.sum(x, dim=1)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6781,  2.1414, -1.1528],\n",
      "        [ 0.4667,  2.6493,  1.2208],\n",
      "        [-0.8366, -1.6180,  0.0561]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.randn(3, 3)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如官方60分钟教程中所说，以_为结尾的，均会改变调用值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6781,  2.1414, -1.1528],\n",
      "        [ 0.4667,  2.6493,  1.2208],\n",
      "        [-0.8366, -1.6180,  0.0561]])\n"
     ]
    }
   ],
   "source": [
    "# add 完成后x的值改变了\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearing",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
